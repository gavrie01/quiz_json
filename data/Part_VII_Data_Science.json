[
    {
       "question": "Why logarithms are so actively used in Data Science, in Neural Networks as an example?",
       "options": ["mostly it happens because variables' values are so small, that i.e. final product of weights can be zeroed by calculation, logarithm resolves the case", "logarithms are not used, derivatives are"],
       "correct_answer": "mostly it happens because variables' values are so small, that i.e. final product of weights can be zeroed by calculation, logarithm resolves the case"
     },
     {
      "question": "Masked Language Modelling it is when ...",
      "options": ["a model masks profanity", "a certain percentage of the words in corpus are randomly chosen to be masked, they are replaced with a special token, such as [MASK], the model predicts these words"],
      "correct_answer": "a certain percentage of the words in corpus are randomly chosen to be masked, they are replaced with a special token, such as [MASK], the model predicts these words"
    },
    {
      "question": "BERT as pre-trained model has ...",
      "options": ["no trained word embeddings vocabulary", "trained word embeddings vocabulary"],
      "correct_answer": "trained word embeddings vocabulary"
    },
    {
      "question": "As of 2022 BERT Large language model has ... parameters",
      "options": ["340 thousand", "340 million", "500"],
      "correct_answer": "340 million"
    },
    {
      "question": "Google has been using your reCAPTCHA selections to label training data since 2011",
      "options": ["Yes", "No way"],
      "correct_answer": "Yes"
    },
    {
      "question": "BERT was trained on ....",
      "options": ["Wikipedia and Google’s BooksCorpus", "Encyclopedia Britannica", "whole WWW content"],
      "correct_answer": "Wikipedia and Google’s BooksCorpus"
    },
    {
      "question": "In NN weight and bias are",
      "options": ["non-trainable paramters", "hyper parameters", "trainable parameters"],
      "correct_answer": "trainable parameters"
    },
    {
      "question": "Transformers create differential weights signaling which words in a sentence are the most ... to further process",
      "options": ["critical", "useless"],
      "correct_answer": "critical"
    },
    {
      "question": "The Transformer architecture consists of ...",
      "options": ["Encoder and Decoder", "Encoder", "Decoder"],
      "correct_answer": "Encoder and Decoder"
    },
    {
      "question": "What is GLUE",
      "options": ["glue is glue", "General Language Understanding Evaluation benchmark, it consists of 9 tasks"],
      "correct_answer": "General Language Understanding Evaluation benchmark, it consists of 9 tasks"
    },
    {
      "question": "Example of unigram ...",
      "options": ["['A love natural language processing']", "['I', 'love', 'natural', 'language', 'processing']"],
      "correct_answer": "['I', 'love', 'natural', 'language', 'processing']"
    },
    {
      "question": "Example of trigram ...",
      "options": ["['I love natural', 'love natural language', 'natural language processing']", "['I love language']"],
      "correct_answer": "['I love natural', 'love natural language', 'natural language processing']"
    },
    {
      "question": "Context is important for models' training",
      "options": ["Yes", "No"],
      "correct_answer": "Yes"
    },
    {
      "question": "Provide an example of word sense disambiguation in English",
      "options": ["Book vs Book", "Book vs Reserve", "Book vs Cancel"],
      "correct_answer": "Book vs Book"
    },
    {
      "question": "Provide an example of word sense disambiguation in Russian or think about it in your native language",
      "options": ["Коса, Косой, Косить", "Маленький, Меньше, Малыш"],
      "correct_answer": "Коса, Косой, Косить"
    },
    {
      "question": "Distillation in NLP is ...",
      "options": ["knowledge transfer from heavy, complex model to light and smaller one", "knowledge transfer from light and small model to heavy and complex, it will be working faster"],
      "correct_answer": "knowledge transfer from heavy, complex model to light and smaller one"
    },
    {
      "question": "Min-Max Scaling scales the data to a specific range (e.g., between 0 and 1) based on the minimum and maximum values and ...",
      "options": ["is sensitive to Outliers", "is not sensitive to outliers"],
      "correct_answer": "is sensitive to Outliers"
    },
    {
      "question": "Standardization (Z-score Normalization) transforms the data to have a mean of 0 and a standard deviation of 1 and is senstitve to Outliers",
      "options": ["Yes, sensitive esp on small data sets", "No, not sensitive"],
      "correct_answer": "Yes, sensitive esp on small data sets"
    },
    {
      "question": "Robust Scaling scales the data using the median and IQR and is less affected by extreme data",
      "options": ["Yes", "No"],
      "correct_answer": "Yes"
    },
    {
      "question": "Which data type has limited precision in python",
      "options": ["float is limited by 64 bits", "string of course", "integer"],
      "correct_answer": "float is limited by 64 bits"
    },
    {
      "question": "General Idea of Transfer Learning is ...",
      "options": ["Pre-training on large amount of cheap data + Post-training on specific data", "Pre-training on specific data + Post-training on large and cheap data"],
      "correct_answer": "Pre-training on large amount of cheap data + Post-training on specific data"
    },
    {
      "question": "Main idea Cross-Validation",
      "options": ["", ""],
      "correct_answer": ""
    },
    {
      "question": "GPT-3 has ... of trainable parameters",
      "options": ["175 billion", "110 million"],
      "correct_answer": "175 billion"
    },
    {
      "question": "Transformers use ... to connect Neurons between each other",
      "options": ["attention", "ingnoring", "laughing"],
      "correct_answer": "attention"
    },
    {
      "question": "Attention in Transformers, i.e. GPT, BERT",
      "options": ["allows the model to learn different aspects of relationships within the input data, how 'tight' words are with each other", "pays attention to profanity"],
      "correct_answer": "allows the model to learn different aspects of relationships within the input data, how 'tight' words are with each other"
    },
    {
      "question": "Tokens like <CLS>, <POS>, <SEP>, <UNK>, etc in the context of transformer-based models like BERT  are used for ...",
      "options": ["model marks up the text, prepares it for further processing, these tokens provide additional info on structure", "everything what is marked wit tokens is removed from the model"],
      "correct_answer": "model marks up the text, prepares it for further processing, these tokens provide additional info on structure"
    },
    {
      "question": "Query, Key, Value Vectors<br>Attention Scores<br>Weighted Sum of Vectors' Values<br> all this about",
      "options": ["Normalization mechanism to ignore negative sense of the text being processed", "Attention mechanism in Transformer neural network"],
      "correct_answer": "Attention mechanism in Transformer neural network"
    },
    {
      "question": "NLU is ...",
      "options": ["Natural Language Understanding, deals with the ability of a computer system to comprehend, interpret, and derive meaning from human language", "THe same as NLP"],
      "correct_answer": "Natural Language Understanding, deals with the ability of a computer system to comprehend, interpret, and derive meaning from human language"
    },
    {
      "question": "In the context of multi-head attention in transformers, a 'head' refers to",
      "options": ["independently learned set of parameters that are used to compute attention scores", "there is term 'body', not 'head' for the purpose"],
      "correct_answer": ""
    },
    {
      "question": "Deepl has such a precise quality of translation because it uses ...",
      "options": ["neural machine translation models", "Deepl has a secret model, self developed"],
      "correct_answer": "neural machine translation models"
    },
    {
      "question": "The goal of early stopping is to prevent overfitting and improve generalization performance by monitoring the model's performance on a validation dataset during training and stopping the training process when ....",
      "options": ["performance stops improving or starts degrading", "performance stucks on the same level"],
      "correct_answer": "performance stops improving or starts degrading"
    },
    {
      "question": "CUDA is ...",
      "options": ["Compute Unified Device Architecture, parallel computing platform and API model developed by NVIDIA", "You mean barracuda, a fish?"],
      "correct_answer": "Compute Unified Device Architecture, parallel computing platform and API model developed by NVIDIA"
    },
    {
      "question": "If you do not have GPU on your PC but need to train your model q u i c k l y, your choice is ...",
      "options": ["Google Colaboratory", "Go to the store and buy it"],
      "correct_answer": "Google Colaboratory"
    },
    {
      "question": "",
      "options": ["", ""],
      "correct_answer": ""
    },
    {
      "question": "",
      "options": ["", ""],
      "correct_answer": ""
    }
       
   
   ]
   