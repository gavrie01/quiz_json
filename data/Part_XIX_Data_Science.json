[
    {
       "question": "Python: for loop and else, can for loop use else statement?",
       "options": ["no, else belongs to conditional if", "yes, of course"],
       "correct_answer": "yes, of course"
     },
     {
      "question": "Python, can we use float in this construction: 'for x in range(0.5, 5.5, 0.5):'",
      "options": ["yes", "no, only integers"],
      "correct_answer": "no, only integers"
    },
    {
      "question": "print( (1.1 + 2.2) == 3.3 )",
      "options": ["true", "false"],
      "correct_answer": "false"
    },
    {
      "question": "PyTorch, NN: stack of layers is a ...",
      "options": ["model", "block"],
      "correct_answer": "block"
    },
    {
      "question": "PyTorch, NN: stack of blocks is a ...",
      "options": ["model", "block", "composite block"],
      "correct_answer": "model"
    },
    {
      "question": "In the context of a transformer's encoder, 'MLP' typically refers to the ...",
      "options": ["Maximum Layer Precision", "Multi-Layer Perceptron"],
      "correct_answer": "Multi-Layer Perceptron"
    },
    {
      "question": "Multi-Layer Perceptron is the same as Multi-Layer Feedforward Network",
      "options": ["yes", "no"],
      "correct_answer": "yes"
    },
    {
      "question": "PyTorch, NN: The ViT model breaks down an input image into fixed-size non-overlapping ..., and each .... is linearly embedded into a lower-dimensional vector",
      "options": ["patches / patch", "tensors / tensor"],
      "correct_answer": "patches / patch"
    },
    {
      "question": "Pytorch, NN: ViT is ...",
      "options": ["Vision Transformer", "Visual Interpretation Technology"],
      "correct_answer": "Vision Transformer"
    },
    {
      "question": "PyTorch, NN: stack of blocks is a Model, in this context Model is the same as ...",
      "options": ["Model", "Architecture", "Composite Model"],
      "correct_answer": "Architecture"
    },
    {
      "question": "PyTorch, NN, 2024: Which model variants has ViT",
      "options": ["Tiny / Small / Medium / Large", "Base / Large / Huge"],
      "correct_answer": "Base / Large / Huge"
    },
    {
      "question": "PyTorch, NN: Residual connections mainly help mitigate ....",
      "options": ["model's overfitting", "vanishing gradient problem", "model'S underfitting"],
      "correct_answer": "vanishing gradient problem"
    },
    {
      "question": "PyTorch, NN: Without the residual connections, a large part of the training signal would get lost during ...",
      "options": ["loss function calculation", "back-propagation", "optimization"],
      "correct_answer": "back-propagation"
    },
    {
      "question": "PyTorch, NN: Residual connections are typically used in ... neural networks where the vanishing gradient problem may impede the training process",
      "options": ["very deep", "shallow", "moderate"],
      "correct_answer": "very deep"
    },
    {
      "question": "PyTorch, NN: very deep neural networks may contain thousands of layers",
      "options": ["yes", "no"],
      "correct_answer": "yes"
    },
    {
      "question": "PyTorch, NN: residual connections in neural networks involve the use of ... during the training process",
      "options": ["derivative", "gradient"],
      "correct_answer": "derivative"
    },
    {
      "question": "Most competitive neural sequence transduction models have an ... structure",
      "options": ["encoder-decoder", "encoder", "decoder", "predictive"],
      "correct_answer": "encoder-decoder"
    },
    {
      "question": "it's common to see neural networks designed with ... in both the encoder and decoder.",
      "options": ["multiple layers", "one layer", "no layers, since encoding and decoding belong to input and output layers"],
      "correct_answer": "multiple layers"
    },
    {
      "question": "An attention function can be described as mapping a query and a set of key-value pairs to an output. What is 'query' here?",
      "options": ["vector that you want to focus on", "sql query"],
      "correct_answer": "vector that you want to focus on"
    },
    {
      "question": "In NLP: a 'token' can be a word, a subword, or even a character, depending on how the tokenization process is performed",
      "options": ["yes", "no"],
      "correct_answer": "yes"
    },
    {
      "question": "A simplistic or straight forward approach to assigning labels or categories to data points without considering more sophisticated methods or nuances",
      "options": ["hypothesis labeling system", "naive labeling system"],
      "correct_answer": "naive labeling system"
    },
    {
      "question": "NN, PyTorch, ViT: Dropout, when used applied after every dense layer (in general, but with some exception)",
      "options": ["yes", "no"],
      "correct_answer": "yes"
    },
    {
      "question": "Dense Layer in NN is a layer, where neurons are connected as:",
      "options": ["each one from the previous layer with each one from current, etc", "neurons from previous layer are connected to neurons from current layer based on maximum weights"],
      "correct_answer": "each one from the previous layer with each one from current, etc"
    },
    {
      "question": "PyTorch supports two execution modes:",
      "options": ["eager / graph", "lazy / tree", "now / later"],
      "correct_answer": "eager / graph"
    },
    {
      "question": "PyTorch: In ... mode, operators in a model are immediately executed as they are encountered",
      "options": ["eager", "graph"],
      "correct_answer": "eager"
    },
    {
      "question": "PyTorch: In graph mode, operators are first synthesized into a graph, which will then be compiled and executed as a whole",
      "options": ["yes", "no"],
      "correct_answer": "yes"
    },
    {
      "question": "GPT model, LLMs: Vectors DB is a result of trained Neural Network",
      "options": ["yes", "no"],
      "correct_answer": "yes"
    },
    {
      "question": "PyTorch: ... mode typically delivers higher performance",
      "options": ["Graph", "Eager"],
      "correct_answer": "Graph"
    },
    {
      "question": "PyTorch: ... mode is easier to use, more suitable for ML researchers",
      "options": ["Eager", "Graph"],
      "correct_answer": "Eager"
    },
    {
      "question": "PyTorch: default mode of execution for PyTorch model is ...",
      "options": ["eager", "graph"],
      "correct_answer": "eager"
    },
    {
      "question": "GPT models, LLM: Vector in vectors DB contains semantic and contextual info learnt during training phase",
      "options": ["yes", "semantic only"],
      "correct_answer": "yes"
    },
    {
      "question": "GPT is a type of autoregressive language model based on the Transformer architecture",
      "options": ["yes", "no"],
      "correct_answer": "yes"
    },
    {
      "question": "The ... architecture is commonly used in seq2seq models",
      "options": ["encoder", "encoder-decoder", "encoder-decoder-encoder"],
      "correct_answer": "encoder-decoder"
    },
    {
      "question": "",
      "options": ["", "", ""],
      "correct_answer": ""
    },
    {
      "question": "",
      "options": ["", "", ""],
      "correct_answer": ""
    }
       
         
   ]
   