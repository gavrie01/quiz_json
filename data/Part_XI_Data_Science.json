[
    {
       "question": "Which machine learning layer from the following cannot be parallelized on a GPU?",
       "options": ["convolutional", "recurrent, it requires one by by input that can not be parallelized", "hidden layer"],
       "correct_answer": "recurrent, it requires one by by input that can not be parallelized"
     },
    {
      "question": "For arabic language in neural language network right-to-left generation is used",
      "options": ["yes", "no, there is still left-to-right generation like in english, the text is transformed on last hidden layer"],
      "correct_answer": "yes"
     }, 
     {
      "question": "An n-gram model can be considered",
      "options": ["an example of deep learning", "a statistical language model", "a neural language network"],
      "correct_answer": "a statistical language model"
    },
    {
      "question": "Can GitHub be used for a model training?",
      "options": ["no, only wikipedia like resources can be used", "yes, LMs can be trained like this"],
      "correct_answer": "yes, LMs can be trained like this"
    },
    {
      "question": "GPT deals with next word predictions tasks",
      "options": ["yes", "no"],
      "correct_answer": "yes"
    },
    {
      "question": "softmax function normalize attention scores so they sum up to ",
      "options": ["1", "100"],
      "correct_answer": "1"
    },  
    {
      "question": "Language Model trained with spanish will perform well with english",
      "options": ["true", "false"],
      "correct_answer": "false"
    },
    {
      "question": "What does this code do?<br>tokenized = tokenizer('The cat is on the mat', return_tensors='pt')<br>",
      "options": ["puts the data into the format of a PyTorch tensor which is used as the input for a Transformer model", "yes, i agree"],
      "correct_answer": "puts the data into the format of a PyTorch tensor which is used as the input for a Transformer model"
    },
    {
      "question": "For a given sequence of text with 3 tokens and language model, the perplexity is calculated as 4. What is the probability calculated for the whole sequence?",
      "options": ["1/3", "1/4", "1/64: A perplexity of 4 shows that the average per-token probability is 1/4, so the probability of a three token sequence is 1/4 * 1/4 * 1/4 which is 1/64"],
      "correct_answer": "1/64: A perplexity of 4 shows that the average per-token probability is 1/4, so the probability of a three token sequence is 1/4 * 1/4 * 1/4 which is 1/64"
    },
    {
      "question": "For the sentences<br>'I want to watch a movie' and 'I checked the time on my watch'<br> what the vectors could be?",
      "options": ["[0.4, 0.1] and [0.1, 0.4]", "[0.4, 0.1] and [0.21, 0.40]"],
      "correct_answer": "[0.4, 0.1] and [0.1, 0.4]"
    },
    {
      "question": "How words are vectorized?",
      "options": ["This is model's global task done via math in language models", "vectorized? no, AI works with texts"],
      "correct_answer": "This is model's global task done via math in language models"
    },
    {
      "question": "",
      "options": ["", ""],
      "correct_answer": ""
    },
    {
      "question": "",
      "options": ["", ""],
      "correct_answer": ""
    },
    {
      "question": "",
      "options": ["", ""],
      "correct_answer": ""
    },
    {
      "question": "",
      "options": ["", ""],
      "correct_answer": ""
    },
    {
      "question": "",
      "options": ["", ""],
      "correct_answer": ""
    },
    {
      "question": "",
      "options": ["", ""],
      "correct_answer": ""
    },
    {
      "question": "",
      "options": ["", ""],
      "correct_answer": ""
    },
    {
      "question": "",
      "options": ["", ""],
      "correct_answer": ""
    },
    {
      "question": "",
      "options": ["", ""],
      "correct_answer": ""
    },
    {
      "question": "",
      "options": ["", ""],
      "correct_answer": ""
    },
    {
      "question": "",
      "options": ["", ""],
      "correct_answer": ""
    },
    {
      "question": "",
      "options": ["", ""],
      "correct_answer": ""
    },
    {
      "question": "",
      "options": ["", ""],
      "correct_answer": ""
    },
    {
      "question": "",
      "options": ["", ""],
      "correct_answer": ""
    },
    {
      "question": "",
      "options": ["", ""],
      "correct_answer": ""
    },
    {
      "question": "",
      "options": ["", ""],
      "correct_answer": ""
    },
    {
      "question": "",
      "options": ["", ""],
      "correct_answer": ""
    },
    {
      "question": "",
      "options": ["", ""],
      "correct_answer": ""
    },
    {
      "question": "",
      "options": ["", ""],
      "correct_answer": ""
    },
    {
      "question": "",
      "options": ["", ""],
      "correct_answer": ""
    },
    {
      "question": "",
      "options": ["", ""],
      "correct_answer": ""
    },
    {
      "question": "",
      "options": ["", ""],
      "correct_answer": ""
    },
    {
      "question": "",
      "options": ["", ""],
      "correct_answer": ""
    },
    {
      "question": "",
      "options": ["", ""],
      "correct_answer": ""
    }
       
   
   ]
   