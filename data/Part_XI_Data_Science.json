[
    {
       "question": "Which machine learning layer from the following cannot be parallelized on a GPU?",
       "options": ["convolutional", "recurrent, it requires one by by input that can not be parallelized", "hidden layer"],
       "correct_answer": "recurrent, it requires one by by input that can not be parallelized"
     },
    {
      "question": "For arabic language in neural language network right-to-left text generation is used",
      "options": ["yes", "no, there is still left-to-right generation like in english, the text is transformed on last hidden layer"],
      "correct_answer": "yes"
     }, 
     {
      "question": "An n-gram model can be considered",
      "options": ["an example of deep learning", "a statistical language model", "a neural language network"],
      "correct_answer": "a statistical language model"
    },
    {
      "question": "Can GitHub be used for a model training?",
      "options": ["no, only wikipedia like resources can be used", "yes, LMs can be trained like this"],
      "correct_answer": "yes, LMs can be trained like this"
    },
    {
      "question": "GPT deals with next word predictions tasks",
      "options": ["yes", "no"],
      "correct_answer": "yes"
    },
    {
      "question": "softmax function normalizes attention scores so they sum up to ",
      "options": ["1", "100"],
      "correct_answer": "1"
    },  
    {
      "question": "Language Model trained with spanish will perform well with english",
      "options": ["true", "false"],
      "correct_answer": "false"
    },
    {
      "question": "What does this code do?<br>tokenized = tokenizer('The cat is on the mat', return_tensors='pt')<br>",
      "options": ["puts the data into the format of a PyTorch tensor which is used as the input for a Transformer model", "yes, i agree"],
      "correct_answer": "puts the data into the format of a PyTorch tensor which is used as the input for a Transformer model"
    },
    {
      "question": "For a given sequence of text with 3 tokens and language model, the perplexity is calculated as 4. What is the probability calculated for the whole sequence?",
      "options": ["1/3", "1/4", "1/64: A perplexity of 4 shows that the average per-token probability is 1/4, so the probability of a three token sequence is 1/4 * 1/4 * 1/4 which is 1/64"],
      "correct_answer": "1/64: A perplexity of 4 shows that the average per-token probability is 1/4, so the probability of a three token sequence is 1/4 * 1/4 * 1/4 which is 1/64"
    },
    {
      "question": "For the sentences<br>'I want to watch a movie' and 'I checked the time on my watch'<br> what the vectors could be here?",
      "options": ["[0.4, 0.1] and [0.1, 0.4]", "[0.4, 0.1] and [0.21, 0.40]"],
      "correct_answer": "[0.4, 0.1] and [0.1, 0.4]"
    },
    {
      "question": "How words are vectorized?",
      "options": ["This is model's global task done via math in language models", "vectorized?", "no, AI works with texts"],
      "correct_answer": "This is model's global task done via math in language models"
    },
    {
      "question": "... in context of Bayesian Inference is the updated probability of the hypothesis or event after considering both the prior probability and the likelihood",
      "options": ["posterior probability", "relative frequency", "nominal"],
      "correct_answer": "posterior probability"
    },
    {
      "question": "Example of prompt ...",
      "options": ["The capital of France is ...", "Write a paragraph about the capital of France"],
      "correct_answer": "The capital of France is ..."
    },
    {
      "question": "Generally speaking, ChatGPT works this way:<br>1. splits text with built-in model's tokenizer into tokens<br>2. encodes tokens to get token_id<br>3. Runs them through the model itself and gets probability of the next token_id(s)<br>4. Applies sampling algorithm to predict next word<br>5.Decodes all token_ids, so human can read",
      "options": ["Yes", "Not exactly, it looks up for a highest probability token_ids, at least"],
      "correct_answer": "Not exactly, it looks up for a highest probability token_ids, at least"
    },
    {
      "question": "Can I use a tokenizer from one transformer model with another model, especially if they are compatible in terms of architecture and vocabulary?",
      "options": ["In many cases yes, but you have to be careful with such things like compatibility in terms of vocabulary, text parsing, positional embeddings, etc", "Yes, why not, there are still texts, still python code, why not"],
      "correct_answer": "In many cases yes, but you have to be careful with such things like compatibility in terms of vocabulary, text parsing, positional embeddings, etc"
    },
    {
      "question": "Why do some language models use a special padding token?",
      "options": ["to process multiple inputs of different length", "it gives model some time to think before to generate another yet gibberish"],
      "correct_answer": "to process multiple inputs of different length"
    },
    {
      "question": "NN with Language models understand the text that they are processing the way humans do",
      "options": ["yes", "no"],
      "correct_answer": "no"
    },
    {
      "question": "Transformers have limit on the length of their inputs",
      "options": ["yes", "no"],
      "correct_answer": "yes"
    },
    {
      "question": "How are word vectors and parameters in a Transformer based language model learned?",
      "options": ["Linear regression", "Gradient descent by learning from data"],
      "correct_answer": "Gradient descent by learning from data"
    },
    {
      "question": "What is Hallucination in LLM?",
      "options": ["not sure I get what you mean ", "a confident response by an AI that does not seem to be justified by its training data"],
      "correct_answer": "a confident response by an AI that does not seem to be justified by its training data"
    },
    {
      "question": "Can biased data cause hallucinations in LLM?",
      "options": ["yes", "no"],
      "correct_answer": "yes"
    },
    {
      "question": "Can overfitting to training data cause hallucinations in LMM",
      "options": ["yes", "no"],
      "correct_answer": "yes"
    },
    {
      "question": "Can complex architecture of LLM cause hallucinations?",
      "options": ["yes", "no"],
      "correct_answer": "yes"
    },
    {
      "question": "When you ask ChatGPT a question and it returns i.e. an empty response it is example of multiple ... it has",
      "options": ["hallucinations", "bugs"],
      "correct_answer": "bugs"
    },
    {
      "question": "Sometimes LLM tries to find patterns in data where they do not exist and finds them, it is ...",
      "options": ["hallucination", "this is how generative AI works, it is ok"],
      "correct_answer": "hallucination"
    },
    {
      "question": "Sometimes LLMs can create fictional details when answering our questions",
      "options": ["definitely", "it is impossible, they are not humans"],
      "correct_answer": "definitely"
    },
    {
      "question": "Adversarial Testing it is a method ...",
      "options": ["for systematically evaluating an ML model with the intent of learning how it behaves when provided with malicious or inadvertently harmful input", "another name for happy path testing which is performed by prompt engineer"],
      "correct_answer": "for systematically evaluating an ML model with the intent of learning how it behaves when provided with malicious or inadvertently harmful input"
    },
    {
      "question": "The more complicated and bigger Language Model is the more human alike behavior it has",
      "options": ["no, it still can be untruthful, toxic, or simply not helpful to the user", "yes, it has bigger vocabulary, this is the most important thing"],
      "correct_answer": "no, it still can be untruthful, toxic, or simply not helpful to the user"
    },
    {
      "question": "As of 2023 and in AI expansion context, human extinction is a current or hypothetical risk?",
      "options": ["current", "hypothetical"],
      "correct_answer": "hypothetical"
    },
    {
      "question": "In Python, True - False == ?",
      "options": ["1", "-1", "error"],
      "correct_answer": "1"
    },
    {
      "question": "Every time when I modify the string, Python creates a new string and assigns it to the variable",
      "options": ["true", "false"],
      "correct_answer": "true"
    },
    {
      "question": "In NLP, a dialogue is a sequence of ... each a single contribution from one speaker to the dialogue",
      "options": ["blocks", "turns", "units", "tokens"],
      "correct_answer": "turns"
    },
    {
      "question": "self-attention mechanism is implemented in NN as ...",
      "options": ["layer", "special type of token"],
      "correct_answer": "layer"
    },
    {
      "question": "in LSTM,  the purpose of this gate is to delete information from the context that is no longer needed",
      "options": ["remove gate", "forget gate", "forgive and forget gate"],
      "correct_answer": "forget gate"
    },
    {
      "question": "When gradients are eventually driven to zero, a situation called the ... problem",
      "options": ["cheshire gradients", "vanishing gradients"],
      "correct_answer": "vanishing gradients"
    }
       
   
   ]
   