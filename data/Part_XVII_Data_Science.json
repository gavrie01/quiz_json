[
    {
       "question": "In NLP, a word vector is often represented as a ... vector",
       "options": ["row", "sparse", "dense"],
       "correct_answer": "row"
     },
     {
      "question": "Generally speaking a row vector contains frequency of the word across documents in corpora",
      "options": ["yes", "no"],
      "correct_answer": "yes"
    },
    {
      "question": "To measure similarity of two words ... is used to measure an angle between their two vectors",
      "options": ["sine", "cosine"],
      "correct_answer": "cosine"
    },
    {
      "question": "The document frequency (DT) of a term is the number of ... it occurs in",
      "options": ["sentences", "documents", "semantic frames"],
      "correct_answer": "documents"
    },
    {
      "question": "In NLP, the lowest weight of 1 is assigned to a term that occur in all the documents",
      "options": ["yes", "yes, but the lowest is 0"],
      "correct_answer": "yes"
    },
    {
      "question": "... measures how important a term is across a collection of documents (corpus)",
      "options": ["Inverse Document Frequency", "Document Frequency"],
      "correct_answer": "Inverse Document Frequency"
    },
    {
      "question": "print(r'slashnhello'), what is the output and why?",
      "options": ["'slashnhello', 'r' creates a raw string literal", "error", "new line, 'hello'"],
      "correct_answer": "'slashnhello', 'r' creates a raw string literal"
    },
    {
      "question": "PMI (pointwise mutual information) is one of important concepts in NLP, it measures how often two events x and y occur, compared with what we would expect if the were independent",
      "options": ["yes", "yes, but not in NLP, it is related to CNN, pixels processing"],
      "correct_answer": "yes"
    },
    {
      "question": "Negative PMI values are ... reliable",
      "options": ["reliable", "unreliable", "unreliable unlesss our corpora are enormous"],
      "correct_answer": "unreliable unlesss our corpora are enormous"
    },
    {
      "question": "PPMI (Positive PMI) replaces all negative PMI values with 0",
      "options": ["yes", "yes, but it replaces with 1", "yes, but P in PPMI means 'probable'"],
      "correct_answer": "yes"
    },
    {
      "question": "Very rare words tend to have  very ... PMI values",
      "options": ["low", "high"],
      "correct_answer": "high"
    },
    {
      "question": "In NLP context, there is such thing like the multidimensional version of mean: a single vector that has the minimum sum of squared distances to each of the vectors in the set",
      "options": ["centroid document vector", "vector space"],
      "correct_answer": "centroid document vector"
    },
    {
      "question": "Word2Vec is a model used in NLP, it represents words as continuous vector spaces",
      "options": ["yes", "no"],
      "correct_answer": "yes"
    },
    {
      "question": "Sparse vectors work better than dense vectors in any NLP task",
      "options": ["yes", "no, it is vice versa dense better than sparse"],
      "correct_answer": "no, it is vice versa dense better than sparse"
    },
    {
      "question": "... co-occurrence, it is when two words have similar neighbors",
      "options": ["First-order", "Second-order"],
      "correct_answer": "Second-order"
    },
    {
      "question": "In NLP context, the parallelogram method is in general too ... to model human cognitive process",
      "options": ["complex", "simple"],
      "correct_answer": "simple"
    },
    {
      "question": "When a model allocates resources (job or credit) unfairly to different groups, i.e. it downweights women when hiring programmers",
      "options": ["allocational harm", "hallucination"],
      "correct_answer": "allocational harm"
    },
    {
      "question": "When gendered terms become more gendered in embedding space than they were in the input text statistics, this is ..",
      "options": ["bias amplification", "allocational bias"],
      "correct_answer": "bias amplification"
    },
    {
      "question": "Vector semantic models fall into 2 classes: ",
      "options": ["sparse / dense", "first-word co-occurrence / second-word co-occurrence"],
      "correct_answer": "sparse / dense"
    },
    {
      "question": "The idea of vector semantics arose out of research (linguistics, computer science, psychology) in the ...",
      "options": ["2000s", "1950s", "1990s"],
      "correct_answer": "1950s"
    },
    {
      "question": "In neural language models as activation function ... is commonly used, this function is almost similar, but always performs better",
      "options": ["sigmoid", "tanh"],
      "correct_answer": "tanh"
    },
    {
      "question": "The ... function maps input values from the entire real number line to the range (-1, 1) and is sigmoidal in shape",
      "options": ["tanh", "sine", "cosine"],
      "correct_answer": "tanh"
    },
    {
      "question": "Using of ... function 'avoids' vanishing gradient problem",
      "options": ["linear", "ReLU"],
      "correct_answer": "ReLU"
    },
    {
      "question": "... is a very simple neural unit that has a binary output and does not have a non-linear activation function",
      "options": ["Perceptron", "Multitron"],
      "correct_answer": "Perceptron"
    },
    {
      "question": "If A = 1 and B = 1, A XOR B = ...",
      "options": ["1", "0"],
      "correct_answer": "0"
    },
    {
      "question": "Is it possible to build a perceptron to compute logical XOR? Why?",
      "options": ["no, because XOR is not a linearly separable function", "yes, it is possible to build for OR and AND"],
      "correct_answer": "no, because XOR is not a linearly separable function"
    },
    {
      "question": "While XOR can not be computed by perceptron it can be computed by layered neural network",
      "options": ["yes", "no"],
      "correct_answer": "yes"
    },
    {
      "question": "1e-3 equals ...",
      "options": ["1000", "0.001", "-2e"],
      "correct_answer": "0.001"
    },
    {
      "question": "Early stopping prevents overfitting",
      "options": ["yes", "no"],
      "correct_answer": "yes"
    },
    {
      "question": "Train for longer time helps to reduce underfitting",
      "options": ["yes", "no"],
      "correct_answer": "yes"
    },
    {
      "question": "Tweaking a learning rate is a good approach to handle ... ",
      "options": ["overfitting", "underfitting"],
      "correct_answer": "underfitting"
    },
    {
      "question": "In machine learning 'just right' it is when ...",
      "options": ["train and test 'loss over learning rate' curves are similar and close to X axe enough", "train is higher than test", "both could be lower on the graph, there is still enough space till X axe"],
      "correct_answer": "train and test 'loss over learning rate' curves are similar and close to X axe enough"
    },
    {
      "question": "... - optimizer in PyTorch",
      "options": ["Luis", "Adam", "Billy"],
      "correct_answer": "Adam"
    },
    {
      "question": "A  data type contains all whole non-negative numbers from 0 to 255",
      "options": ["uint8", "integer"],
      "correct_answer": "uint8"
    },
    {
      "question": "uint8 data type is mostly used in ...",
      "options": ["graphics", "video", "audio"],
      "correct_answer": "graphics"
    }
         
   ]
   