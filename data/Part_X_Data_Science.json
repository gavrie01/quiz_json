[
    {
       "question": "Is 2 a rational number?",
       "options": ["yes, because it can be represented as 2 / 1", "no, it is natural number"],
       "correct_answer": "yes, because it can be represented as 2 / 1"
     },
     {
      "question": "If division by 0 is not defined, let us just remove this 0 it spoils everything",
      "options": ["simply speaking we can not, 0 is a significant part of numeral system and is used in i.e. linear algebra, geometry", "even do not start this discussion"],
      "correct_answer": "simply speaking we can not, 0 is a significant part of numeral system and is used in i.e. linear algebra, geometry"
    },
    {
      "question": "How many rational numbers between 0 and 1",
      "options": ["infinitely many", "1", "non of the above"],
      "correct_answer": "infinitely many"
    },
    {
      "question": "Rational and Irrational numbers make up Real Numbers system",
      "options": ["yes", "no"],
      "correct_answer": "yes"
    },
    {
      "question": "Square root of 2 is ... number",
      "options": ["irrational", "rational", "irrational and real"],
      "correct_answer": "irrational and real"
    },
    {
      "question": "2x + y -1 = 0, is it linear equation",
      "options": ["yes", "no"],
      "correct_answer": "yes"
    },
    {
      "question": "x*x + y*y = 1, is it what we call function in math?",
      "options": ["yes, it is so called 'circle' function", "no, because an x value corresponds here to two y values, while in true function x has just one y value"],
      "correct_answer": "no, because an x value corresponds here to two y values, while in true function x has just one y value"
    },
    {
      "question": "F(x) = x, is it linear function?",
      "options": ["yes, why not", "no, it does not look like a function"],
      "correct_answer": "yes, why not"
    },
    {
      "question": "F(x) = x, slope of this function equals to ...",
      "options": ["1", "there is no slope"],
      "correct_answer": "1"
    },
    {
      "question": "What is the solution for: sqrt of -9",
      "options": ["there is no solution, -9 is negative", "sqrt of (9 * (-1)) = 3*i, where i is imaginary number"],
      "correct_answer": "sqrt of (9 * (-1)) = 3*i, where i is imaginary number"
    },
    {
      "question": "What does store large texts in SQL?",
      "options": ["BLOB", "CLOB"],
      "correct_answer": "CLOB"
    },
    {
      "question": "Causal Language Model ...",
      "options": ["measures probability of next word in a sequence", "generates causal texts based on wikipedia articles"],
      "correct_answer": "measures probability of next word in a sequence"
    },
    {
      "question": "A little bit of english, what is plural from 'corpus'?",
      "options": ["corpuses, what a strange question", "corpora"],
      "correct_answer": "corpora"
    },
    {
      "question": "What does the 'n' in n-gram represent?",
      "options": ["number of words for prediction", "number of words to train the model"],
      "correct_answer": "number of words for prediction"
    },
    {
      "question": "Perplexity is ... evaluation",
      "options": ["Intrinsic", "Extrinsic"],
      "correct_answer": "Intrinsic"
    },
    {
      "question": "Low perplexity means ... ",
      "options": ["language model gives high probabilities for the texts", "language model gives low probabilities for the texts"],
      "correct_answer": "language model gives high probabilities for the texts"
    },
    {
      "question": "Laplace Smoothing is used in NLP to handle ... ",
      "options": ["the issue of zero probabilities for unseen events", "the issue of stop-words for unseen events"],
      "correct_answer": "the issue of zero probabilities for unseen events"
    },
    {
      "question": "The idea of Laplace Smoothing is to 'smooth' the probability distribution by  ... ... ... to the count of each event, including those that have not been observed in the training data",
      "options": ["adding a sigmoid function to probabilities", "adding a small constant (commonly 1)"],
      "correct_answer": "adding a small constant (commonly 1)"
    },
    {
      "question": "What does have a higher computational cost?",
      "options": ["beam search", "greedy search"],
      "correct_answer": "beam search"
    },
    {
      "question": "Beam search picks up the word with the k highest probabilities from the dictionary",
      "options": ["yes, this how beams are created in fact", "no, it selects just one, the highest"],
      "correct_answer": "yes, this how beams are created in fact"
    },
    {
      "question": "Greedy search is just a constrained version of Beam Search, where k == 1",
      "options": ["yes", "no"],
      "correct_answer": "yes"
    },
    {
      "question": "ChatGPT uses sampling text generation strategy, this is why when you ask the same question a few times you get different answers",
      "options": ["yes", "no"],
      "correct_answer": "yes"
    },
    {
      "question": "Sampling text generation strategy it is when ...",
      "options": ["algorithm chooses between a number of words with relatively close probability", "no, sampling it is about images only"],
      "correct_answer": "algorithm chooses between a number of words with relatively close probability"
    },
    {
      "question": "OOV token means",
      "options": ["Out Of Vocabulary", "Overly Optimistic Vocabulary"],
      "correct_answer": "Out Of Vocabulary"
    },
    {
      "question": "What is the difference in probabilistic and deterministic algorithms of text generation in Data Science?",
      "options": ["deterministic produce the same output, while probabilistic introduce randomness or uncertainty into the modeling process", "determinism introduces randomness or uncertainty into the modeling process, while probabilistic produce the same output"],
      "correct_answer": "deterministic produce the same output, while probabilistic introduce randomness or uncertainty into the modeling process"
    },
    {
      "question": "In neural language model relevance scores can be converted into probability with ... function",
      "options": ["softmax", "softmin", "hardmax"],
      "correct_answer": "softmax"
    },
    {
      "question": "Auto-regressive language generation is based on the assumption that the probability distribution of a word sequence can be decomposed into the product of conditional next word distributions",
      "options": ["Yes", "No, auto-regression is related to linear models only, they can not be used in NLP"],
      "correct_answer": "Yes"
    },
    {
      "question": "... search selects the word with the highest probability as its next word",
      "options": ["Sampling", "Greedy"],
      "correct_answer": "Greedy"
    },
    {
      "question": "The major drawback of greedy search though is that it ...",
      "options": ["misses high probability words hidden behind a low probability word", "misses low probability words hidden behind a high probability word"],
      "correct_answer": "misses high probability words hidden behind a low probability word"
    },
    {
      "question": "BeamSearch text is less surprizing for humans",
      "options": ["Yes", "No"],
      "correct_answer": "Yes"
    },
    {
      "question": "That is the big problem when sampling word sequences: the models often generate ...",
      "options": ["incoherent gibberish", "too scientific content, only for PhD"],
      "correct_answer": "incoherent gibberish"
    },
    {
      "question": "top-p and top-K sampling seem to produce more fluent text than traditional greedy - and beam search on open-ended language generation",
      "options": ["agree", "disagree"],
      "correct_answer": "agree"
    },
    {
      "question": "When a model encounters a word that is not part of its vocabulary during training or evaluation, it replaces that word with the ... token.",
      "options": ["UNK", "NEW"],
      "correct_answer": "UNK"
    },
    {
      "question": "The self-attention mechanism in a neural language network allows the model",
      "options": ["to weigh the importance of different words in a sequence when making predictions", "to weigh the importance of different words in a sequence when normalizing text"],
      "correct_answer": "to weigh the importance of different words in a sequence when making predictions"
    },
    {
      "question": "In Neural Language Model higher levels deal with more complex reasoning while lower layer deal with basic syntax",
      "options": ["yes", "no"],
      "correct_answer": "yes"
    }
      
       
   
   ]
   